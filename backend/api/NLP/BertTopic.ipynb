{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (21.2.4)\n",
      "Requirement already satisfied: install in /Users/robby/Library/Python/3.9/lib/python/site-packages (1.3.5)\n",
      "Requirement already satisfied: hdbscan in /Users/robby/Library/Python/3.9/lib/python/site-packages (0.8.29)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/robby/Library/Python/3.9/lib/python/site-packages (from hdbscan) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/robby/Library/Python/3.9/lib/python/site-packages (from hdbscan) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /Users/robby/Library/Python/3.9/lib/python/site-packages (from hdbscan) (1.2.1)\n",
      "Requirement already satisfied: cython>=0.27 in /Users/robby/Library/Python/3.9/lib/python/site-packages (from hdbscan) (0.29.33)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/robby/Library/Python/3.9/lib/python/site-packages (from hdbscan) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/robby/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/robby/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /Users/robby/Library/Python/3.9/lib/python/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/robby/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/robby/Library/Python/3.9/lib/python/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/robby/Library/Python/3.9/lib/python/site-packages (from nltk) (2022.10.31)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://maartengr.github.io/BERTopic/algorithm/algorithm.html#1-embed-documents\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "\n",
    "\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic represenations\n",
    "  # nr_topics=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "docs = fetch_20newsgroups(subset='all')['data']\n",
    "# topic_model = BERTopic(nr_topics=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m topics, probabilities \u001b[39m=\u001b[39m topic_model\u001b[39m.\u001b[39mfit_transform(docs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_model' is not defined"
     ]
    }
   ],
   "source": [
    "topics, probabilities = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distr, topic_token_distr = topic_model.approximate_distribution(docs, calculate_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load(\"topic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.save(\"topic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x2fa67a3d0>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERTopic.load(\"topic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_number(text):\n",
    "  topics = topic_model.find_topics(text, top_n=1)\n",
    "  return topics[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_topic_words(text):\n",
    "  topic_number = get_topic_number(text)\n",
    "  topics = topic_model.get_topic(topic_number),topic_number\n",
    "  topic_names = ','.join([topic for topic, freq in topics[0]][:4])\n",
    "  return topic_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'motif,unix,bindings'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_representative_topic_words(\"\"\"I'm sorry, but \"mtld\" is not a recognizable term or acronym in English. Can you please provide more context or clarify what you are referring to?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Topic  Count                            Name       CustomName\n",
       " 0    193     18  193_42_tiff_tiff6_significance  42, tiff, tiff6,\n",
       " 193)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word = \"I really want to go get some popcorn when we go to the movies later\"\n",
    "# word = \"I wonder if we will be able to get to the movies on time to watch startreck?\"\n",
    "get_representative_topic_words(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = topic_model.generate_topic_labels(nr_words=3,\n",
    "  topic_prefix=False,\n",
    "  word_length=15,\n",
    "  separator=\", \")\n",
    "topic_model.set_topic_labels(topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>CustomName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>6111</td>\n",
       "      <td>-1_think_information_david_article</td>\n",
       "      <td>think, information, david</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "      <td>0_pitchers_pitcher_batting_pitching</td>\n",
       "      <td>pitchers, pitcher, batting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>483</td>\n",
       "      <td>1_encryption_crypto_cryptography_encrypted</td>\n",
       "      <td>encryption, crypto, cryptography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>416</td>\n",
       "      <td>2_investigation_fbi_evidence_murder</td>\n",
       "      <td>investigation, fbi, evidence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>407</td>\n",
       "      <td>3_firearm_firearms_handgun_guns</td>\n",
       "      <td>firearm, firearms, handgun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>208</td>\n",
       "      <td>16</td>\n",
       "      <td>208_icons_iconize_icon_program</td>\n",
       "      <td>icons, iconize, icon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>209</td>\n",
       "      <td>16</td>\n",
       "      <td>209_omnipotent_omnipotence_creates_existance</td>\n",
       "      <td>omnipotent, omnipotence, creates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>210</td>\n",
       "      <td>16</td>\n",
       "      <td>210_tires_tire_tread_wheels</td>\n",
       "      <td>tires, tire, tread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>211</td>\n",
       "      <td>15</td>\n",
       "      <td>211_orthodox_coptic_ecumenical_nestorians</td>\n",
       "      <td>orthodox, coptic, ecumenical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>212</td>\n",
       "      <td>15</td>\n",
       "      <td>212_pens_bruins_penguins_islanders</td>\n",
       "      <td>pens, bruins, penguins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                          Name  \\\n",
       "0       -1   6111            -1_think_information_david_article   \n",
       "1        0    888           0_pitchers_pitcher_batting_pitching   \n",
       "2        1    483    1_encryption_crypto_cryptography_encrypted   \n",
       "3        2    416           2_investigation_fbi_evidence_murder   \n",
       "4        3    407               3_firearm_firearms_handgun_guns   \n",
       "..     ...    ...                                           ...   \n",
       "209    208     16                208_icons_iconize_icon_program   \n",
       "210    209     16  209_omnipotent_omnipotence_creates_existance   \n",
       "211    210     16                   210_tires_tire_tread_wheels   \n",
       "212    211     15     211_orthodox_coptic_ecumenical_nestorians   \n",
       "213    212     15            212_pens_bruins_penguins_islanders   \n",
       "\n",
       "                           CustomName  \n",
       "0           think, information, david  \n",
       "1          pitchers, pitcher, batting  \n",
       "2    encryption, crypto, cryptography  \n",
       "3        investigation, fbi, evidence  \n",
       "4          firearm, firearms, handgun  \n",
       "..                                ...  \n",
       "209              icons, iconize, icon  \n",
       "210  omnipotent, omnipotence, creates  \n",
       "211                tires, tire, tread  \n",
       "212      orthodox, coptic, ecumenical  \n",
       "213            pens, bruins, penguins  \n",
       "\n",
       "[214 rows x 4 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1 is non relevant topics\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.custom_labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtld(tokens, threshold=0.72):\n",
    "    \"\"\"\n",
    "    Calculate the Measure of Textual Lexical Diversity (MTLD) score for a given list of tokens.\n",
    "    Generates  groups to calculate the type-token ratio (TTR) determined by local diversity + threshold.\n",
    "    Takes the average of the forward and backward to genreate the MTLD score.\n",
    "    Args:\n",
    "        tokens: A list of tokens (words) in the language sample.\n",
    "        threshold: The minimum type-token ratio (TTR) required to maintain lexical diversity.\n",
    "    Returns:\n",
    "        The MTLD score for the language sample.\n",
    "    \"\"\"\n",
    "    ttr, factor_count, word_count = 1.0, 1, len(tokens)\n",
    "    stack = []\n",
    "    for token in tokens:\n",
    "        stack.append(token)\n",
    "        ttr = len(set(stack)) / len(stack)\n",
    "        if ttr < threshold:\n",
    "            stack = []\n",
    "            factor_count += 1\n",
    "    mtld_score = word_count / factor_count\n",
    "    reversed_tokens = list(reversed(tokens))\n",
    "    ttr, factor_count, word_count = 1.0, 1, len(reversed_tokens)\n",
    "    stack = []\n",
    "    for token in reversed_tokens:\n",
    "        stack.append(token)\n",
    "        ttr = len(set(stack)) / len(stack)\n",
    "        if ttr < threshold:\n",
    "            stack = []\n",
    "            factor_count += 1\n",
    "    reversed_mtld_score = word_count / factor_count\n",
    "    return (mtld_score + reversed_mtld_score) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mattr(tokens, window_size):\n",
    "    \"\"\"\n",
    "    Calculate a lexical diversity score using a moving average of type-token ratios (MATTR).\n",
    "    Args:\n",
    "        tokens: A list of tokens (words) in the language sample.\n",
    "        window_size: The size of the window used for calculating TTRs.\n",
    "    Returns:\n",
    "        The MATTR score for the language sample.\n",
    "    \"\"\"\n",
    "    ttrs = []\n",
    "    for i in range(len(tokens) - window_size + 1):\n",
    "        window = tokens[i:i+window_size]\n",
    "        ttr = len(set(window)) / window_size\n",
    "        ttrs.append(ttr)\n",
    "    mattr_score = sum(ttrs) / len(ttrs)\n",
    "    return mattr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jennafrens/lexical_diversity/blob/master/lexical_diversity.py\n",
    "import string\n",
    "\n",
    "# Global trandform for removing punctuation from words\n",
    "remove_punctuation = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# HD-D internals\n",
    "\n",
    "# x! = x(x-1)(x-2)...(1)\n",
    "def factorial(x):\n",
    "    if x <= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return x * factorial(x - 1)\n",
    "\n",
    "# n choose r = n(n-1)(n-2)...(n-r+1)/(r!)\n",
    "def combination(n, r):\n",
    "    r_fact = factorial(r)\n",
    "    numerator = 1.0\n",
    "    num = n-r+1.0\n",
    "    while num < n+1.0:\n",
    "        numerator *= num\n",
    "        num += 1.0\n",
    "    return numerator / r_fact\n",
    "\n",
    "# hypergeometric probability: the probability that an n-trial hypergeometric experiment results \n",
    "#  in exactly x successes, when the population consists of N items, k of which are classified as successes.\n",
    "#  (here, population = N, population_successes = k, sample = n, sample_successes = x)\n",
    "#  h(x; N, n, k) = [ kCx ] * [ N-kCn-x ] / [ NCn ]\n",
    "def hypergeometric(population, population_successes, sample, sample_successes):\n",
    "    return (combination(population_successes, sample_successes) *\\\n",
    "            combination(population - population_successes, sample - sample_successes)) /\\\n",
    "            combination(population, sample)\n",
    "    \n",
    "# HD-D implementation\n",
    "def hdd(word_array, sample_size=42.0):\n",
    "    if isinstance(word_array, str):\n",
    "        raise ValueError(\"Input should be a list of strings, rather than a string. Try using string.split()\")\n",
    "    if len(word_array) < 50:\n",
    "        raise ValueError(\"Input word list should be at least 50 in length\")\n",
    "\n",
    "    # Create a dictionary of counts for each type\n",
    "    type_counts = {}\n",
    "    for token in word_array:\n",
    "        token = token.translate(remove_punctuation).lower() # trim punctuation, make lowercase\n",
    "        if token in type_counts:\n",
    "            type_counts[token] += 1.0\n",
    "        else:\n",
    "            type_counts[token] = 1.0\n",
    "    # Sum the contribution of each token - \"If the sample size is 42, the mean contribution of any given\n",
    "    #  type is 1/42 multiplied by the percentage of combinations in which the type would be found.\" (McCarthy & Jarvis 2010)\n",
    "    hdd_value = 0.0\n",
    "    for token_type in type_counts.keys():\n",
    "        contribution = (1.0 - hypergeometric(len(word_array), sample_size, type_counts[token_type], 0.0)) / sample_size\n",
    "        hdd_value += contribution\n",
    "\n",
    "    return hdd_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'newsgroup,newsgroups,groups,split': 11.8125,\n",
       "  'crohn,colitis,dietary,gastroenterology': 11.217948717948719,\n",
       "  'obesity,dieting,rebound,diet': 12.545454545454547,\n",
       "  'airfare,airfares,tickets,ticket': 12.592592592592593,\n",
       "  'espn,sportschannel,espn2,televised': 11.875,\n",
       "  'hypocrisy,hypocritical,hypocrite,jim': 11.748251748251748,\n",
       "  'stereo,audio,soundbase,speakers': 11.97043010752688,\n",
       "  'scientific,scientists,studies,scientist': 12.727272727272727,\n",
       "  'interrupts,interrupt,irq7,ports': 13.416666666666666,\n",
       "  'morality,morals,moral,ethics': 13.882352941176471},\n",
       " 12.378847004689035)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ex_text = \"\"\"\"\n",
    "re, here are five sentences on five random topics:\n",
    "\n",
    "Coffee: Coffee is one of the most widely consumed beverages in the world, and it is enjoyed for its rich flavor and energizing effects. From a simple cup of black coffee to more complex creations like cappuccinos and lattes, coffee is a staple of many people's daily routines. However, excessive coffee consumption can lead to negative health effects like anxiety and insomnia, so it's important to consume in moderation.\n",
    "Travel: Traveling allows us to experience new cultures, try new foods, and see the world from a different perspective. Whether it's backpacking through Europe or taking a road trip through the United States, travel can be a transformative experience that broadens our horizons and helps us appreciate the beauty and diversity of our planet.\n",
    "Sports: Sports have the ability to bring people together and foster a sense of community and belonging. Whether it's playing on a team or watching a game with friends, sports can provide a source of enjoyment and camaraderie. However, it's important to remember that winning isn't everything and that sportsmanship and fair play are essential components of any game.\n",
    "Music: Music has the power to evoke emotions and memories, transport us to different places and times, and connect us with other people. Whether it's listening to a favorite song or attending a live concert, music can be a source of joy and inspiration. Additionally, learning to play an instrument or sing can provide a creative outlet and help develop new skills.\n",
    "Artificial Intelligence: Artificial Intelligence (AI) is an exciting field that has the potential to revolutionize the way we live and work. From self-driving cars to intelligent personal assistants like Siri and Alexa, AI technology is rapidly advancing and changing the way we interact with machines. However, there are also concerns about the potential negative impacts of AI, such as the displacement of jobs and the loss of privacy. It's important to continue to research and develop AI technology in a responsible and ethical manner.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sent_ex_text = sent_tokenize(ex_text)\n",
    "\n",
    "def text_to_diversity_per_topic(text, diversity_function):\n",
    "  tokenized_text = sent_tokenize(text)\n",
    "  window_to_sentence = {}\n",
    "  for idx, i in enumerate(tokenized_text):\n",
    "    topic_for_sentence = get_representative_topic_words(i)\n",
    "    window_to_sentence[topic_for_sentence] = {*window_to_sentence.get(topic_for_sentence, {}), idx}\n",
    "  diversity = {}\n",
    "  total_diversity_scores = 0\n",
    "  # calculate the diversity for each group\n",
    "  for topic, sentence_indexes in window_to_sentence.items():\n",
    "    bag_of_words = ' '.join([tokenized_text[idx] for idx in sentence_indexes])\n",
    "    diversity_score = diversity_function(bag_of_words)\n",
    "    diversity[topic] = diversity_score\n",
    "    total_diversity_scores += diversity_score\n",
    "\n",
    "\n",
    "  return diversity, total_diversity_scores/len(diversity)\n",
    "text_to_diversity_per_topic(ex_text,mtld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
